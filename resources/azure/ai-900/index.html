
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../assets/dl-logo-big-512x512-color.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.0.5">
    
    
      
        <title>Microsoft Azure Fundamentals Certification - DrLiptons</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.a617204b.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.9204c3b2.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  <link rel="stylesheet" href="../../../overrides/assets/stylesheets/main.d9227bb8.min.css">

  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="indigo" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#microsoft-azure-fundamentals-certification" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="DrLiptons" class="md-header__button md-logo" aria-label="DrLiptons" data-md-component="logo">
      
  <img src="../../../assets/dl-logo-big-512x512-color.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            DrLiptons
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Microsoft Azure Fundamentals Certification
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="" data-md-color-primary="indigo" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 1 7 7c0 2.38-1.19 4.47-3 5.74V17a1 1 0 0 1-1 1H9a1 1 0 0 1-1-1v-2.26C6.19 13.47 5 11.38 5 9a7 7 0 0 1 7-7M9 21v-1h6v1a1 1 0 0 1-1 1h-4a1 1 0 0 1-1-1m3-17a5 5 0 0 0-5 5c0 2.05 1.23 3.81 3 4.58V16h4v-2.42c1.77-.77 3-2.53 3-4.58a5 5 0 0 0-5-5z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 0-7 7c0 2.38 1.19 4.47 3 5.74V17a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1v-2.26c1.81-1.27 3-3.36 3-5.74a7 7 0 0 0-7-7M9 21a1 1 0 0 0 1 1h4a1 1 0 0 0 1-1v-1H9v1z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../.." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../about/" class="md-tabs__link">
      About
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../wip/" class="md-tabs__link">
      In Progress
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../reference/" class="md-tabs__link">
        Reference
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../resource/" class="md-tabs__link">
        Resource
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="DrLiptons" class="md-nav__button md-logo" aria-label="DrLiptons" data-md-component="logo">
      
  <img src="../../../assets/dl-logo-big-512x512-color.png" alt="logo">

    </a>
    DrLiptons
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../wip/" class="md-nav__link">
        In Progress
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_2" type="checkbox" id="__nav_4_2" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4_2">
          Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/get-live-financial-data/" class="md-nav__link">
        Get Live Financial Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/flight-deals/" class="md-nav__link">
        Flight Deals
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/alien-invaders/" class="md-nav__link">
        Alien Invaders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/auto-job-app/" class="md-nav__link">
        Automated Job Application
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/price-tracker/" class="md-nav__link">
        Price Tracker
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/password-manager/" class="md-nav__link">
        Password Manager
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/text-2-morse-code/" class="md-nav__link">
        Text2MorseCode
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/stock-news-alerts/" class="md-nav__link">
        Stock News Alert
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/tetris/" class="md-nav__link">
        Tetris Clone
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_3" type="checkbox" id="__nav_4_3" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4_3">
          TensorFlow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="TensorFlow" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          TensorFlow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_3_1" type="checkbox" id="__nav_4_3_1" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4_3_1">
          Plant Disease Detection
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Plant Disease Detection" data-md-level="3">
        <label class="md-nav__title" for="__nav_4_3_1">
          <span class="md-nav__icon md-icon"></span>
          Plant Disease Detection
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/pdd/plant-disease-detection/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/pdd/part-1-create-ml-model/" class="md-nav__link">
        Part 1 - Create ML Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/pdd/part-2-create-app/" class="md-nav__link">
        Part 2 - Desktop App
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/pdd/part-3-make-exe/" class="md-nav__link">
        Part 3 - Make Executable
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/object-detection/" class="md-nav__link">
        Object Detection
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5">
          Resource
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Resource" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Resource
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../resource/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2" type="checkbox" id="__nav_5_2" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_2">
          Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/scikit-learn/" class="md-nav__link">
        Scikit-Learn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/matplotlib/" class="md-nav__link">
        Matplotlib
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/numpy/" class="md-nav__link">
        Numpy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/pandas/" class="md-nav__link">
        Pandas
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/mplfinance/" class="md-nav__link">
        mplfinance
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3" type="checkbox" id="__nav_5_3" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3">
          TensorFlow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="TensorFlow" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          TensorFlow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3_1" type="checkbox" id="__nav_5_3_1" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3_1">
          TensorFlow Exam
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="TensorFlow Exam" data-md-level="3">
        <label class="md-nav__title" for="__nav_5_3_1">
          <span class="md-nav__icon md-icon"></span>
          TensorFlow Exam
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/tf-exam/tf-exam-overview/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3_1_2" type="checkbox" id="__nav_5_3_1_2" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3_1_2">
          Coursera - DeepLearning.AI TensorFlow Developer
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Coursera - DeepLearning.AI TensorFlow Developer" data-md-level="4">
        <label class="md-nav__title" for="__nav_5_3_1_2">
          <span class="md-nav__icon md-icon"></span>
          Coursera - DeepLearning.AI TensorFlow Developer
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/tf-exam/cs-part-1/" class="md-nav__link">
        Part 1 - Introduction to TF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/tf-exam/cs-part-2/" class="md-nav__link">
        Part 2 - CNN in TF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/tf-exam/cs-part-3/" class="md-nav__link">
        Part 3 -  NLP in TF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/tf-exam/cs-part-4/" class="md-nav__link">
        Part 4 - Time Series
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/tf-exam/pass-tensorflow-cert/" class="md-nav__link">
        How I past TensorFlow Exam
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/tf-model-bp/" class="md-nav__link">
        TensorFlow Blueprint
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3_3" type="checkbox" id="__nav_5_3_3" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3_3">
          TensorFlow Models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="TensorFlow Models" data-md-level="3">
        <label class="md-nav__title" for="__nav_5_3_3">
          <span class="md-nav__icon md-icon"></span>
          TensorFlow Models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/model-list/" class="md-nav__link">
        List of Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3_3_2" type="checkbox" id="__nav_5_3_3_2" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3_3_2">
          Neural Network Regressions
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Neural Network Regressions" data-md-level="4">
        <label class="md-nav__title" for="__nav_5_3_3_2">
          <span class="md-nav__icon md-icon"></span>
          Neural Network Regressions
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/simple-regression/" class="md-nav__link">
        NNR - House Prices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/regression-boston-housing/" class="md-nav__link">
        NNR - Boston Housing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/titanic/" class="md-nav__link">
        NNR - Titanic
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3_3_3" type="checkbox" id="__nav_5_3_3_3" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3_3_3">
          Image Classification
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Image Classification" data-md-level="4">
        <label class="md-nav__title" for="__nav_5_3_3_3">
          <span class="md-nav__icon md-icon"></span>
          Image Classification
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/basic-fmnist/" class="md-nav__link">
        Basic Image Classification - FMNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/image-classification-dogs-cats/" class="md-nav__link">
        Image Classification - Dogs & Cats
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/cnn-fmnist/" class="md-nav__link">
        CNN - FMNIST
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3_3_4" type="checkbox" id="__nav_5_3_3_4" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3_3_4">
          Natural Language Processing
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Natural Language Processing" data-md-level="4">
        <label class="md-nav__title" for="__nav_5_3_3_4">
          <span class="md-nav__icon md-icon"></span>
          Natural Language Processing
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/imdb/" class="md-nav__link">
        NLP - Binary-Class IMDB
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/nlp-1-binary/" class="md-nav__link">
        NLP - Binary-Class Disaster Tweets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/nlp-2-multi/" class="md-nav__link">
        NLP - Multi-Class PedMed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/nlp-3-sentiment/" class="md-nav__link">
        NLP - Multi-Class Sentiment Analysis
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3_3_5" type="checkbox" id="__nav_5_3_3_5" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3_3_5">
          Time Series
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Time Series" data-md-level="4">
        <label class="md-nav__title" for="__nav_5_3_3_5">
          <span class="md-nav__icon md-icon"></span>
          Time Series
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/ts-bitcoin/" class="md-nav__link">
        Time Series - Bitcoin Price Forecast
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/ts-weather/" class="md-nav__link">
        Time Series - Weather Forecast
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3_3_6" type="checkbox" id="__nav_5_3_3_6" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3_3_6">
          Miscellaneous
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Miscellaneous" data-md-level="4">
        <label class="md-nav__title" for="__nav_5_3_3_6">
          <span class="md-nav__icon md-icon"></span>
          Miscellaneous
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorflow/models/utilities/" class="md-nav__link">
        Utility Functions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#at-a-glance" class="md-nav__link">
    At a Glance
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#module-1-describe-ai-workloads-and-considerations" class="md-nav__link">
    Module 1 - Describe AI workloads and considerations
  </a>
  
    <nav class="md-nav" aria-label="Module 1 - Describe AI workloads and considerations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#azure-machine-learning-services" class="md-nav__link">
    Azure Machine Learning Services
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anomaly-detection" class="md-nav__link">
    Anomaly Detection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computer-vision" class="md-nav__link">
    Computer Vision
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#natural-language-processing" class="md-nav__link">
    Natural Language Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#knowledge-mining" class="md-nav__link">
    Knowledge Mining
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#challenges-and-risk-with-ai" class="md-nav__link">
    Challenges and Risk with AI
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#responsible-ai" class="md-nav__link">
    Responsible AI
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-2-describe-fundamental-principles-of-machine-learning" class="md-nav__link">
    Model 2 - Describe fundamental principles of machine learning
  </a>
  
    <nav class="md-nav" aria-label="Model 2 - Describe fundamental principles of machine learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#regression-model" class="md-nav__link">
    Regression Model
  </a>
  
    <nav class="md-nav" aria-label="Regression Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate-model" class="md-nav__link">
    Evaluate Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classification" class="md-nav__link">
    Classification
  </a>
  
    <nav class="md-nav" aria-label="Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate-model_1" class="md-nav__link">
    Evaluate Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clustering" class="md-nav__link">
    Clustering
  </a>
  
    <nav class="md-nav" aria-label="Clustering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate-model_2" class="md-nav__link">
    Evaluate Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-3-" class="md-nav__link">
    Model 3 -
  </a>
  
    <nav class="md-nav" aria-label="Model 3 -">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#analyze-images-with-computer-vision-service" class="md-nav__link">
    Analyze images with Computer Vision service
  </a>
  
    <nav class="md-nav" aria-label="Analyze images with Computer Vision service">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#azure-resources-for-computer-vision" class="md-nav__link">
    Azure resources for Computer Vision
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#analyzing-images-with-computer-vision-service" class="md-nav__link">
    Analyzing images with Computer Vision service
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classify-images-with-the-custom-vision-service" class="md-nav__link">
    Classify images with the Custom Vision service
  </a>
  
    <nav class="md-nav" aria-label="Classify images with the Custom Vision service">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#azure-resources-for-custom-vision" class="md-nav__link">
    Azure resources for Custom Vision
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-evaluation" class="md-nav__link">
    Model Evaluation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-the-model-for-prediction" class="md-nav__link">
    Using the model for prediction
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detect-objects-in-images-with-the-custom-vision-service" class="md-nav__link">
    Detect objects in images with the Custom Vision service
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detect-and-analyze-faces-with-the-face-service" class="md-nav__link">
    Detect and analyze faces with the Face service
  </a>
  
    <nav class="md-nav" aria-label="Detect and analyze faces with the Face service">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#face-detection" class="md-nav__link">
    Face detection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#facial-analysis" class="md-nav__link">
    Facial analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#facial-recognition" class="md-nav__link">
    Facial recognition
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#read-text-with-the-computer-vision-service" class="md-nav__link">
    Read text with the Computer Vision service
  </a>
  
    <nav class="md-nav" aria-label="Read text with the Computer Vision service">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ocr-api" class="md-nav__link">
    OCR API
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#read-api" class="md-nav__link">
    Read API
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#form-recognizer-service" class="md-nav__link">
    Form Recognizer Service
  </a>
  
    <nav class="md-nav" aria-label="Form Recognizer Service">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#azure-resources-to-access-form-recognizer-services" class="md-nav__link">
    Azure resources to access Form Recognizer services
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-the-pre-built-receipt-model" class="md-nav__link">
    Using the pre-built receipt model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-4-not-cleaned" class="md-nav__link">
    Model 4 -  (NOT CLEANED)
  </a>
  
    <nav class="md-nav" aria-label="Model 4 -  (NOT CLEANED)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#natural-language-processing-not-cleaned" class="md-nav__link">
    Natural Language Processing (NOT CLEANED)
  </a>
  
    <nav class="md-nav" aria-label="Natural Language Processing (NOT CLEANED)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-analytics-techniques-not-cleaned" class="md-nav__link">
    Text Analytics Techniques (NOT CLEANED)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#azure-resources-for-the-language-service-not-cleaned" class="md-nav__link">
    Azure resources for the Language service  (NOT CLEANED)
  </a>
  
    <nav class="md-nav" aria-label="Azure resources for the Language service  (NOT CLEANED)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#language-detection-not-cleaned" class="md-nav__link">
    Language Detection (NOT CLEANED)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentiment-analysis-not-cleaned" class="md-nav__link">
    Sentiment Analysis (NOT CLEANED)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-phrase-extraction-not-cleaned" class="md-nav__link">
    Key phrase extraction (NOT CLEANED)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#entity-recognition-not-cleaned" class="md-nav__link">
    Entity recognition (NOT CLEANED)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recognize-and-synthesize-speech" class="md-nav__link">
    Recognize and synthesize speech
  </a>
  
    <nav class="md-nav" aria-label="Recognize and synthesize speech">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#speech-recognition" class="md-nav__link">
    Speech recognition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speech-synthesis" class="md-nav__link">
    Speech synthesis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#azure-resources-for-the-speech-service" class="md-nav__link">
    Azure resources for the Speech service
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#translate-text-and-speech" class="md-nav__link">
    Translate text and speech
  </a>
  
    <nav class="md-nav" aria-label="Translate text and speech">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#literal-and-semantic-translation" class="md-nav__link">
    Literal and semantic translation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-and-speech-translation" class="md-nav__link">
    Text and speech translation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#azure-resources-for-translation" class="md-nav__link">
    Azure resources for translation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conversational-language-understanding" class="md-nav__link">
    Conversational Language Understanding
  </a>
  
    <nav class="md-nav" aria-label="Conversational Language Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#azure-resources-for-conversational-language-understanding" class="md-nav__link">
    Azure resources for Conversational Language Understanding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#authoring" class="md-nav__link">
    Authoring
  </a>
  
    <nav class="md-nav" aria-label="Authoring">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#creating-intents" class="md-nav__link">
    Creating intents
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#creating-entities" class="md-nav__link">
    Creating entities
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-the-model" class="md-nav__link">
    Training the model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#predicting" class="md-nav__link">
    Predicting
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build-a-bot-with-language-service-and-azure-bot-service" class="md-nav__link">
    Build a bot with Language Service and Azure Bot Service
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#anomaly-detection_1" class="md-nav__link">
    Anomaly Detection
  </a>
  
    <nav class="md-nav" aria-label="Anomaly Detection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-anomaly-detector-works" class="md-nav__link">
    How Anomaly Detector works
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-format" class="md-nav__link">
    Data format
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-consistency-recommendations" class="md-nav__link">
    Data consistency recommendations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-detection" class="md-nav__link">
    Batch detection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#real-time-detection" class="md-nav__link">
    Real-time detection
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#explore-knowledge-mining" class="md-nav__link">
    Explore knowledge mining
  </a>
  
    <nav class="md-nav" aria-label="Explore knowledge mining">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#azure-cognitive-search" class="md-nav__link">
    Azure Cognitive Search
  </a>
  
    <nav class="md-nav" aria-label="Azure Cognitive Search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#azure-cognitive-search-features" class="md-nav__link">
    Azure Cognitive Search features
  </a>
  
    <nav class="md-nav" aria-label="Azure Cognitive Search features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#built-in-skills" class="md-nav__link">
    Built in skills
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#indexes" class="md-nav__link">
    Indexes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                

<h1 id="microsoft-azure-fundamentals-certification">Microsoft Azure Fundamentals Certification</h1>
<p><img alt="image" src="../../../assets/images/coming-soon.png" /></p>
<p>The Microsoft Azure AI Fundamentals Certification (AI-900) ...</p>
<h2 id="at-a-glance"><b>At a Glance</b></h2>
<details class="example" open="open">
<summary>At a Glance</summary>
<ul>
<li><a href="#intro">Introduction</a></li>
<li><a href="#module-1">Model 1 - Describe AI workloads and considerations</a></li>
<li><a href="#module-2">Model 2 - Describe fundamental principles of machine learning</a></li>
<li><a href="#module-3">Model 3 - Describe features of computer vision workloads</a></li>
<li><a href="#module-4">Model 4 - Describe features of Natural Language Processing (NLP) workloads</a></li>
<li><a href="#module-5">Model 5 - features of conversational AI workloads</a></li>
<li><a href="#exam">Prepare for the Exam</a></li>
</ul>
</details>
<p><br></p>
<p><a name='intro')></a></p>
<h2 id="introduction"><strong>Introduction</strong></h2>
<p>Skills to be measured</p>
<ul>
<li>Describe AI workloads and considerations (15-20%)</li>
<li>Describe fundamental principles of machine learning on Azure (30-35%)</li>
<li>Describe features of computer vision workloads on Azure (15-20%)</li>
<li>Describe features of Natural Language Processing (NLP) workloads on Azure (15-20%)</li>
<li>Describe features of conversational AI workloads on Azure (15-20%)</li>
</ul>
<p><a name='module-1')></a></p>
<h2 id="module-1-describe-ai-workloads-and-considerations"><strong>Module 1 - Describe AI workloads and considerations</strong></h2>
<p><strong>Artificial Intelligence</strong> or <strong>AI</strong> is a software that imitates human behaviors and capabilities, such as</p>
<ul>
<li><strong>Machine learning</strong> - is the foundation of AI which is to <strong>teach a computer model to make prediction and draw conclusions</strong> from data</li>
<li><strong>Anomaly detection</strong> - the capability to automatically <strong>detect errors</strong> or <strong>unusual activity</strong></li>
<li><strong>Computer vision</strong> - the capability to <strong>interpret the world visually</strong> through cameras, video, and images</li>
<li><strong>Natural language processing</strong> - the capability to <strong>interpret and respond to written or spoken language</strong></li>
<li><strong>Knowledge mining</strong> - the capability to <strong>extract information</strong> from large volumes of data to create a searchable knowledge store.</li>
</ul>
<h3 id="azure-machine-learning-services">Azure Machine Learning Services</h3>
<p>is a cloud-based platform for creating, managing, and publishing machine learning models</p>
<table>
<thead>
<tr>
<th align="left">Feature</th>
<th align="left">Capability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Automated machine learning</td>
<td align="left">enables non-experts to quickly create an effective machine learning model from data</td>
</tr>
<tr>
<td align="left">Azure Machine Learning designer</td>
<td align="left">a graphical interface enabling no-code development of machine learning solutions</td>
</tr>
<tr>
<td align="left">Data and compute management</td>
<td align="left">cloud-based data storage and compute resources to run data experiment code</td>
</tr>
<tr>
<td align="left">Pipelines</td>
<td align="left">a tool to orchestrate model training, deployment, and management tasks</td>
</tr>
</tbody>
</table>
<h3 id="anomaly-detection">Anomaly Detection</h3>
<p>is a machine learning based technique that analyzes data over time and identifies unusual changes</p>
<p><strong>Microsoft Azure Anomaly Detector</strong> service provides an application programming interface (API) to create anomaly detection solutions</p>
<h3 id="computer-vision">Computer Vision</h3>
<p>are based on machine learning models that can be applied to visual input from cameras, videos, or images</p>
<table>
<thead>
<tr>
<th align="left">Task</th>
<th align="left">Capability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><strong>Image classification</strong></td>
<td align="left"><img align="left" alt="Image title" src="../../images/image-classification.png" /> classify <strong>(whole) images</strong> based on their contents</td>
</tr>
<tr>
<td align="left"><strong>Object detection</strong></td>
<td align="left"><img align="left" alt="Image title" src="../../images/object-detection.png" />classify individual <strong>objects within an image</strong>, and identify their location with a bounding box</td>
</tr>
<tr>
<td align="left"><strong>Semantic segmentation</strong></td>
<td align="left"><img align="left" alt="Image title" src="../../images/semantic-segmentation.png" />classify <strong>individual pixels in the image (mask)</strong> according to the object they belong</td>
</tr>
<tr>
<td align="left"><strong>Image analysis</strong></td>
<td align="left"><img align="left" alt="Image title" src="../../images/image-analysis.png" />a technique to <strong>extract information</strong> from images, and create "tags" for the image or descriptive captions to summarize the scene shown in the image</td>
</tr>
<tr>
<td align="left"><strong>Face detection, analysis, and recognition</strong></td>
<td align="left"><img align="left" alt="Image title" src="../../images/face-analysis.png" /> <strong>detect human faces in an image</strong> to classify and analyse the details of a person such as age and emotional state</td>
</tr>
<tr>
<td align="left"><strong>Optical character recognition (OCR)</strong></td>
<td align="left"><img align="left" alt="Image title" src="../../images/ocr.png" /><strong>detect and read text</strong> in images</td>
</tr>
</tbody>
</table>
<p>Microsoft Azure services to create computer vision solutions</p>
<table>
<thead>
<tr>
<th align="left">Service</th>
<th align="left">Capability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Computer Vision</td>
<td align="left">analyze images and video, and extract descriptions, tags, objects, and text</td>
</tr>
<tr>
<td align="left">Custom Vision</td>
<td align="left">train custom image classification and object detection models using your own images</td>
</tr>
<tr>
<td align="left">Face</td>
<td align="left">build face detection and facial recognition solutions</td>
</tr>
<tr>
<td align="left">Form Recognizer</td>
<td align="left">extract information from scanned forms and invoices</td>
</tr>
</tbody>
</table>
<h3 id="natural-language-processing">Natural Language Processing</h3>
<p>NLP can:</p>
<ul>
<li><strong>Analyze and interpret text</strong> in documents, email messages, and other sources</li>
<li><strong>Interpret spoken language</strong>, and synthesize speech responses</li>
<li><strong>Automatically translate spoken</strong> or written phrases between languages</li>
<li><strong>Interpret commands</strong> and determine appropriate actions</li>
</ul>
<p>Microsoft Azure services to create natural language processing solutions</p>
<table>
<thead>
<tr>
<th align="left">Service</th>
<th align="left">Capability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Language</td>
<td align="left">a service to access features for <strong>understanding and analyzing text</strong> to <strong>understand spoken or text-based commands</strong></td>
</tr>
<tr>
<td align="left">Translator</td>
<td align="left">a service to <strong>translate text</strong> between more than <strong>60 languages</strong></td>
</tr>
<tr>
<td align="left">Speech</td>
<td align="left">a service to <strong>recognize and synthesize speech</strong>, and to <strong>translate spoken languages</strong></td>
</tr>
<tr>
<td align="left">Azure Bot</td>
<td align="left">a platform for <strong>conversational AI</strong> tha have the capability allow the <strong>software to participate in a conversation</strong></td>
</tr>
</tbody>
</table>
<h3 id="knowledge-mining">Knowledge Mining</h3>
<p>is to <strong>extract information</strong> from large volumes of often unstructured data to <strong>create a searchable knowledge store</strong></p>
<p><strong>Azure Cognitive Search</strong> a <strong>service for building indexes</strong> that can be used for searchable contents. It can be used as a 
built-in AI capabilities of Azure Cognitive Services such as image processing, content extraction, and natural language 
processing to perform knowledge mining of documents.</p>
<h3 id="challenges-and-risk-with-ai">Challenges and Risk with AI</h3>
<table>
<thead>
<tr>
<th align="left">Challenge or Risk</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><strong>Bias</strong> can affect results</td>
<td align="left">A loan-approval model discriminates by gender due to bias in the data with which it was trained</td>
</tr>
<tr>
<td align="left"><strong>Errors</strong> may cause harm</td>
<td align="left">An autonomous vehicle experiences a system failure and causes a collision</td>
</tr>
<tr>
<td align="left"><strong>Data</strong> could be <strong>exposed</strong></td>
<td align="left">A medical diagnostic bot is trained using sensitive patient data, which is stored insecurely</td>
</tr>
<tr>
<td align="left">Solutions may <strong>not work for everyone</strong></td>
<td align="left">A home automation assistant provides no audio output for visually impaired users</td>
</tr>
<tr>
<td align="left">Users must <strong>trust</strong> a complex system</td>
<td align="left">An AI-based financial tool makes investment recommendations - what are they based on?</td>
</tr>
<tr>
<td align="left"><strong>Who's liable</strong> for AI-driven decisions</td>
<td align="left">An innocent person is convicted of a crime based on evidence from facial recognition – who's responsible?</td>
</tr>
</tbody>
</table>
<h3 id="responsible-ai">Responsible AI</h3>
<p>a <strong>guideline</strong> of a set of <strong>6 principles</strong>, to avoid AI applications from unintended negative consequences</p>
<ul>
<li><strong>Fairness</strong> - make <strong>predictions without any bias</strong> such as based on gender, ethnicity, or other factors</li>
<li><strong>Reliability and safety</strong> - the AI system must go through <strong>rigorous testing</strong> and <strong>approval</strong> before it can be in use</li>
<li><strong>Privacy and security</strong> - AI system must be <strong>secure</strong>, and it must respect the <strong>privacy of individuals</strong></li>
<li><strong>Inclusiveness</strong> - <strong>engage</strong> and empower <strong>people from wide aspects</strong> of society</li>
<li><strong>Transparency</strong> - users should be made <strong>aware of the purpose of the system</strong>, its working protocols, and processes, and its expected limitations</li>
<li><strong>Accountability</strong> - <strong>developers</strong> of AI-based solution should <strong>work within a framework of governance</strong> and organizational principles that ensure it meets <strong>ethical and legal standards</strong>.</li>
</ul>
<p><a name='module-2')></a></p>
<h2 id="model-2-describe-fundamental-principles-of-machine-learning"><strong>Model 2 - Describe fundamental principles of machine learning</strong></h2>
<p><strong>Machine Learning</strong> is a technique that uses mathematics and statistics to create a <strong>model that can predict</strong> unknown values</p>
<h3 id="regression-model">Regression Model</h3>
<p><strong>Regression</strong> is a form of machine learning that is used to <strong>predict a numeric label</strong> based on an item's features. Regression is an example of a <strong>supervised machine learning</strong> technique in which you train a model using data that includes both the features and known values for the label, so that the model learns to fit the feature combinations to the label.</p>
<p><strong>Normalize Data</strong>: transform the numeric values so that the values are all on a <strong>similar scale</strong>
<strong>Splitting data</strong>: into two sets enables you to <strong>compare</strong> the labels that the model <strong>predicts with the actual</strong> known labels in the original dataset.</p>
<h4 id="evaluate-model">Evaluate Model</h4>
<ul>
<li><strong>Mean Absolute Error (MAE)</strong>: The <strong>average difference between predicted values and true values</strong>. This value is based on the same units as the label, in this case dollars. The <strong>lower</strong> this value is, the <strong>better</strong> the model is predicting.</li>
<li><strong>Root Mean Squared Error (RMSE)</strong>: The <strong>square root of the mean squared difference between predicted and true values</strong>. The result is a metric based on the same unit as the label (dollars). When compared to the MAE (above), a <strong>larger difference</strong> indicates <strong>greater variance</strong> in the individual errors (for example, with some errors being very small, while others are large).</li>
<li><strong>Relative Squared Error (RSE)</strong>: A relative metric <strong>between 0 and 1</strong> based on the <strong>square of the differences between predicted and true values</strong>. The <strong>closer to 0</strong> this metric is, the <strong>better</strong> the model is performing. Because this metric is relative, it can be used to compare models where the labels are in different units.</li>
<li><strong>Relative Absolute Error (RAE)</strong>: A relative metric <strong>between 0 and 1</strong> based on the <strong>absolute differences between predicted and true values</strong>. The <strong>closer to 0</strong> this metric is, the <strong>better</strong> the model is performing. Like RSE, this metric can be used to compare models where the labels are in different units.</li>
<li><strong>Coefficient of Determination (R2)</strong>: This metric is more commonly referred to as R-Squared, and <strong>summarizes how much of the variance between predicted and true values</strong> is explained by the model. The <strong>closer to 1</strong> this value is, the <strong>better</strong> the model is performing.</li>
</ul>
<h3 id="classification">Classification</h3>
<p><strong>Classification</strong> is a form of machine learning that is used to predict which <strong>category, or class</strong>, an item belongs to. Classification is an example of a <strong>supervised machine learning</strong> technique in which you train a model using data that includes both the features and known values for the label, so that the model learns to fit the feature combinations to the label.</p>
<h4 id="evaluate-model_1">Evaluate Model</h4>
<ul>
<li><strong>Confusion Metrix</strong>: </li>
<li><strong>Accuracy</strong>: The <strong>ratio of correct predictions</strong> (<strong>true positives + true negatives</strong>) to the total number of predictions.</li>
<li><strong>Precision</strong>: The fraction of <strong>positive cases correctly identified</strong> (the <strong>number of true positives divided by the number of true positives plus false positives</strong>).</li>
<li><strong>Recall</strong>: The fraction of the cases classified as <strong>positive that are actually positive</strong> (the <strong>number of true positives divided by the number of true positives plus false negatives</strong>). </li>
<li><strong>F1 Score*: An overall </strong>metric<strong> that essentially </strong>combines precision and recall**.</li>
<li><strong>Receiver Operating Characteristic (ROC curve)</strong>: Another term for recall is <strong>True positive rate</strong>, and it has a corresponding metric named False positive rate, which measures the number of negative cases incorrectly identified as positive compared the number of actual negative cases. The <strong>larger the area under the curve</strong> (which can be any value from 0 to 1), the <strong>better</strong> the model is performing</li>
<li><strong>Area Under the Curve (AUC)</strong>: is a term to describe the <strong>area under the ROC curve</strong>. The area under the <strong>diagonal line</strong> represents an <strong>AUC of 0.5</strong> means that you get around <strong>half of them right</strong>.  If the AUC for your model is <strong>higher</strong> than this for a binary classification model, then the model performs <strong>better</strong> than a random guess.</li>
</ul>
<h3 id="clustering">Clustering</h3>
<p><strong>Clustering</strong> is a form of machine learning that is used to <strong>group similar items into clusters</strong> based on their features. Clustering is an example of <strong>unsupervised machine learning</strong>, in which you train a model to separate items into clusters based purely on their characteristics, or features.</p>
<h4 id="evaluate-model_2">Evaluate Model</h4>
<ul>
<li><strong>Average Distance to Other Center</strong>: This indicates how <strong>close, on average</strong>, each point in the cluster is to the <strong>centroids of all</strong> other clusters.</li>
<li><strong>Average Distance to Cluster Center</strong>: This indicates how <strong>close, on average</strong>, each point in the cluster is to the <strong>centroid of the cluster</strong>.</li>
<li><strong>Number of Points</strong>: The <strong>number of points</strong> assigned to the cluster.</li>
<li><strong>Maximal Distance to Cluster Center</strong>: The maximum of the <strong>distances between each point and the centroid of that point’s cluster</strong>. If this number is <strong>high</strong>, the cluster may be <strong>widely dispersed</strong>. This statistic in combination with the Average Distance to Cluster Center helps you determine the cluster’s spread.</li>
</ul>
<p><br></p>
<p><a name='module-4')></a></p>
<h2 id="model-3-"><strong>Model 3 - </strong></h2>
<h3 id="analyze-images-with-computer-vision-service">Analyze images with Computer Vision service</h3>
<p>Creating solutions that enable <strong>AI applications to see the world and make sense of it</strong>.</p>
<p>Some potential uses for computer vision include:</p>
<ul>
<li><strong>Content Organization</strong>: <strong>Identify</strong> people or objects in photos and <strong>organize them based on that identification</strong>. Photo recognition applications like this are commonly used in photo storage and social media applications.</li>
<li><strong>Text Extraction</strong>: Analyze images and PDF documents that contain text and <strong>extract the text into a structured format</strong>.</li>
<li><strong>Spatial Analysis</strong>: <strong>Identify</strong> people or objects, such as cars, in a space <strong>and map their movement within that space</strong>.</li>
</ul>
<h4 id="azure-resources-for-computer-vision">Azure resources for Computer Vision</h4>
<p>Computer Vision service can be used and created as a resource in an Azure subscription. The following services type can be used for computer vision:</p>
<ul>
<li><strong>Computer Vision</strong>: A specific resource for the Computer Vision service. Use this resource type if you <strong>don't intend to use any other cognitive services</strong>, or if you want to track utilization and costs for your Computer Vision resource separately.</li>
<li><strong>Cognitive Services</strong>: A general cognitive services resource that includes Computer Vision along <strong>with many other cognitive services</strong>; such as Text Analytics, Translator Text, and others. Use this resource type if you plan to use multiple cognitive services and want to simplify administration and development.</li>
</ul>
<p>Whichever type of resource you choose to create, it will provide two pieces of information:</p>
<ul>
<li>A <strong>key</strong> that is used to <strong>authenticate client applications</strong>.</li>
<li>An <strong>endpoint</strong> that provides the <strong>HTTP address</strong> at which your <strong>resource can be accessed</strong>.</li>
</ul>
<h4 id="analyzing-images-with-computer-vision-service">Analyzing images with Computer Vision service</h4>
<p>Computer Vision service can perform a wide range of analytical tasks as following</p>
<ul>
<li><strong>Describing an image</strong> - <strong>analyze an image</strong>, evaluate the objects that are detected, and <strong>generate a human-readable phrase</strong> or sentence that can <strong>describe what was detected</strong> in the image.</li>
<li><strong>Tagging visual features</strong> - can be used to <strong>suggest tags for the image</strong>. </li>
<li><strong>Detecting objects</strong> - <strong>identify common objects</strong> and return what is known as <strong>bounding box coordinates</strong></li>
<li><strong>Detecting brands</strong> - <strong>identify commercial brands</strong> and returns a response (if any) that contains the brand name, a <strong>confidence score</strong> (from 0 to 1 indicating how positive the identification is), and a <strong>bounding box</strong> (coordinates)</li>
<li><strong>Detecting faces</strong> - <strong>detect and analyze human faces in an image</strong>, including the ability to <strong>determine age</strong> and a <strong>bounding box</strong> rectangle for the location of the face(s) using such as the <strong>Face Service</strong></li>
<li><strong>Categorizing an image</strong> - <strong>categorize images based on their content</strong></li>
<li><strong>Detecting domain-specific content</strong> - <strong>categorizing an image</strong> into two specialized domain models (1) <strong>Celebrities</strong> and (2) <strong>Landmarks</strong></li>
<li><strong>Optical character recognition (OCR)</strong> - capabilities to <strong>detect printed and handwritten text</strong> in images.</li>
<li><strong>Detect image types</strong> - for example, identifying <strong>clip art</strong> images or <strong>line drawings</strong>.</li>
<li><strong>Detect image color schemes</strong> - specifically, identifying the dominant foreground, background, and overall <strong>colors in an image</strong>.</li>
<li><strong>Generate thumbnails</strong> - creating <strong>small versions of images</strong>.</li>
<li><strong>Moderate content</strong> - detecting images that <strong>contain adult content</strong> or depict <strong>violent</strong>, <strong>gory scenes</strong>.</li>
</ul>
<h3 id="classify-images-with-the-custom-vision-service">Classify images with the Custom Vision service</h3>
<p>AI systems to identify real-world items based on images. Some potential uses for image classification include:</p>
<ul>
<li><strong>Product identification</strong>: performing <strong>visual searches for specific products</strong> in online searches or even, in-store using a mobile device.</li>
<li><strong>Disaster investigation</strong>: identifying key <strong>infrastructure</strong> for major <strong>disaster preparation</strong> efforts. For example, identifying bridges and roads in aerial images can help disaster relief teams plan ahead in regions that are not well mapped.</li>
<li><strong>Medical diagnosis</strong>: <strong>evaluating images</strong> from X-ray or MRI devices could quickly classify specific issues found as cancerous tumors, or many other <strong>medical conditions</strong> related to medical imaging diagnosis.</li>
</ul>
<h4 id="azure-resources-for-custom-vision">Azure resources for Custom Vision</h4>
<p>use the following types of resource:</p>
<p><strong>Custom Vision</strong>: A dedicated <strong>resource</strong> for the custom vision service, which can be <strong>training, a prediction, or both resources</strong>.
<strong>Cognitive Services</strong>: A general cognitive services resource that <strong>includes Custom Vision</strong> along with many other cognitive services. You can use this type of resource for <strong>training, prediction, or both</strong>.</p>
<h4 id="model-evaluation">Model Evaluation</h4>
<ul>
<li><strong>Precision</strong>: What <strong>percentage of the class predictions made by the model were correct</strong>? For example, if the model predicted that 10 images are oranges, of which eight were actually oranges, then the precision is 0.8 (80%).</li>
<li><strong>Recall</strong>: What percentage of class <strong>predictions did the model correctly identify</strong>? For example, if there are 10 images of apples, and the model found 7 of them, then the recall is 0.7 (70%).</li>
<li><strong>Average Precision (AP) (sometimes Mean Average Precision or mAP)</strong>: An overall metric that takes into account both <strong>precision and recall</strong>.</li>
</ul>
<h4 id="using-the-model-for-prediction">Using the model for prediction</h4>
<p>After being satisfied with its evaluated performance, the model can be used for prediction. The model need to be published and assigned a name (the default is "IterationX", where X is the number of times the model has been trained).
To use the model, client application developers need the following information:</p>
<ul>
<li><strong>Project ID</strong>: The <strong>unique ID</strong> of the Custom Vision project of the trained the model</li>
<li><strong>Model name</strong>: The <strong>name of the model</strong> during publishing</li>
<li><strong>Prediction endpoint</strong>: The <strong>HTTP address of the endpoints</strong> for the prediction resource (not the training resource)</li>
<li><strong>Prediction key</strong>: The <strong>authentication key for the prediction resource</strong> (not the training resource)</li>
</ul>
<h3 id="detect-objects-in-images-with-the-custom-vision-service">Detect objects in images with the Custom Vision service</h3>
<p><strong>Object Detection</strong> is a form of machine learning based computer vision in which a model is trained to recognize individual types of objects in an image</p>
<p>An object detection model can be used to identify the individual objects in an image and return the following information:</p>
<p><img align="left" alt="Image Object Detections" src="../../images/produce-objects.png" /> 
* The <strong>class</strong> of each object identified in the image.
* The <strong>probability score</strong> of the object classification (which you can interpret as the <strong><em>confidence</em></strong> of the predicted class being correct)
* The coordinates of a <strong>bounding box</strong> for each object.</p>
<h3 id="detect-and-analyze-faces-with-the-face-service">Detect and analyze faces with the Face service</h3>
<p>Face detection and analysis is an area of artificial intelligence (AI) in which we use algorithms to <strong>locate and analyze human faces</strong> in images or video content</p>
<h4 id="face-detection">Face detection</h4>
<p>Face detection involves <strong>identifying regions</strong> of an image that <strong>contain a human face</strong>, typically by returning <strong>bounding box coordinates</strong> that form a rectangle around the face</p>
<h4 id="facial-analysis">Facial analysis</h4>
<p>Use <strong>facial landmarks as features</strong> with which to train a machine learning model from which can <strong>infer information about a person</strong>, such as their perceived age or perceived emotional state.</p>
<h4 id="facial-recognition">Facial recognition</h4>
<p>To <strong>identify known individuals</strong> from their <strong>facial features</strong> by using multiple images of each person you want to recognize to train a model so that it can detect those individuals in new images on which it wasn't trained.</p>
<h3 id="read-text-with-the-computer-vision-service">Read text with the Computer Vision service</h3>
<p>The ability to extract text from images.</p>
<ul>
<li><strong>Optical Character Recognition (OCR)</strong> is a model can be trained to <strong>recognize individual shapes</strong> as letters, numerals, punctuation, or other elements of text.</li>
<li><strong>Machine Reading Comprehension (MRC)</strong> an AI system to <strong>read</strong> the text characters and use a semantic model to <strong>interpret</strong> what the text is about.</li>
</ul>
<h4 id="ocr-api">OCR API</h4>
<p>The OCR API is designed for quick extraction of <strong>small amounts of text in images</strong>. It operates synchronously to provide immediate results, and can recognize text in numerous languages. It <strong>operates synchronously</strong> to provide immediate results, and can recognize text in numerous languages.</p>
<p>The OCR method <strong>can have issues with false positives</strong> when the image is considered <strong>text-dominate</strong>.</p>
<p>OCR API returns a hierarchy of information that consists of:</p>
<ul>
<li><strong>Regions</strong> in the image that contain text</li>
<li><strong>Lines</strong> of text in each region</li>
<li><strong>Words</strong> in each line of text</li>
</ul>
<p>For each of these elements, the OCR API also returns <strong>bounding box</strong> coordinates that define a rectangle to indicate the location in the image where the region, line, or word appears.</p>
<h4 id="read-api">Read API</h4>
<p>The Read API is for <strong>scanned documents</strong> that have <strong>a lot of text</strong>. It works <strong>asynchronously</strong> so as not to block your application while it is reading the content and returning results to your application.</p>
<ul>
<li><strong>Pages</strong> - One for each page of text, including information about the page size and orientation. </li>
<li><strong>Lines</strong> - The lines of text on a page. </li>
<li><strong>Words</strong> - The words in a line of text.</li>
</ul>
<p>Each line and word includes <strong>bounding box coordinates</strong> indicating its position on the page.</p>
<h3 id="form-recognizer-service">Form Recognizer Service</h3>
<p>The <strong>Form Recognizer</strong> be used to automate the <strong>processing</strong> of data in documents such <strong>as forms, invoices, and receipts</strong> using OCR with predictive models to interpret form data by:</p>
<ul>
<li>Matching field names to values.</li>
<li>Processing tables of data.</li>
<li>Identifying specific types of field, such as dates, telephone numbers, addresses, totals, and others.</li>
</ul>
<p>Form Recognizer supports automated document processing through:</p>
<ul>
<li>A <strong>pre-built receipt model</strong> that is provided <strong>out-of-the-box</strong>, and is trained to <strong>recognize and extract</strong> data from <strong>sales receipt</strong>s.</li>
<li><strong>Custom models</strong>, extract as <strong>key/value pairs and table data from forms</strong>. Trained using <strong>your own data</strong>, to tailor the model to a specific forms.</li>
</ul>
<h4 id="azure-resources-to-access-form-recognizer-services">Azure resources to access Form Recognizer services</h4>
<ul>
<li><strong>Form Recognizer</strong></li>
<li><strong>Cognitive Services</strong></li>
</ul>
<h4 id="using-the-pre-built-receipt-model">Using the pre-built receipt model</h4>
<p>Currently, the pre-built receipt model is designed to recognize <strong>common receipts</strong>, in <strong>English</strong>, that are common to the USA. </p>
<p>To get the <strong>best results</strong> when using a custom model.
* Images must be <strong>JPEG, PNG, BMP, PDF, or TIFF formats</strong>
* File <strong>size</strong> must be <strong>less than 50 MB</strong>
* Image size between <strong>50 x 50 pixels and 10000 x 10000 pixels</strong>
* For <strong>PDF documents</strong>, <strong>no larger than 17 inches x 17 inches</strong></p>
<p><br></p>
<p><a name='module-4')></a></p>
<h2 id="model-4-not-cleaned"><strong>Model 4 - </strong> (NOT CLEANED)</h2>
<h3 id="natural-language-processing-not-cleaned">Natural Language Processing (NOT CLEANED)</h3>
<p>Analyzing text is a process to evaluate different aspects of a document or phrase, to gain insights into the content of that text.</p>
<h4 id="text-analytics-techniques-not-cleaned">Text Analytics Techniques (NOT CLEANED)</h4>
<p>There are some commonly used techniques that can be used to build software to analyze text, including:</p>
<ul>
<li>Statistical analysis of terms used in the text. For example, removing common "stop words" (words like "the" or "a", which reveal little semantic information about the text), and performing frequency analysis of the remaining words (counting how often each word appears) can provide clues about the main subject of the text.</li>
<li>Extending frequency analysis to multi-term phrases, commonly known as N-grams (a two-word phrase is a bi-gram, a three-word phrase is a tri-gram, and so on).</li>
<li>Applying stemming or lemmatization algorithms to normalize words before counting them - for example, so that words like "power", "powered", and "powerful" are interpreted as being the same word.</li>
<li>Applying linguistic structure rules to analyze sentences - for example, breaking down sentences into tree-like structures such as a noun phrase, which itself contains nouns, verbs, adjectives, and so on.</li>
<li>Encoding words or terms as numeric features that can be used to train a machine learning model. For example, to classify a text document based on the terms it contains. This technique is often used to perform sentiment analysis, in which a document is classified as positive or negative.</li>
<li>Creating vectorized models that capture semantic relationships between words by assigning them to locations in n-dimensional space. This modeling technique might, for example, assign values to the words "flower" and "plant" that locate them close to one another, while "skateboard" might be given a value that positions it much further away.</li>
</ul>
<h3 id="azure-resources-for-the-language-service-not-cleaned">Azure resources for the Language service  (NOT CLEANED)</h3>
<ul>
<li><strong>Language</strong> resource - choose this resource type if you only plan to use natural language processing services, or if you want to manage access and billing for the resource separately from other services.</li>
<li><strong>Cognitive Services</strong> resource - choose this resource type if you plan to use the Language service in combination with other cognitive services, and you want to manage access and billing for these services together.</li>
</ul>
<h4 id="language-detection-not-cleaned">Language Detection (NOT CLEANED)</h4>
<p>Use the language detection capability of the Language service to <strong>identify the language</strong> in which text is written.</p>
<p><strong>Ambiguous or mixed language content</strong> - There may be text that is ambiguous in nature, or that has mixed language content. These situations can present a challenge to the service. An ambiguous content example would be a case where the document contains limited text, or only punctuation. For example, using the service to analyze the text ":-)", results in a value of unknown for the language name and the language identifier, and <strong>a score of NaN</strong> (which is used to indicate not a number).</p>
<h4 id="sentiment-analysis-not-cleaned">Sentiment Analysis (NOT CLEANED)</h4>
<p>This capability is useful for <strong>detecting positive and negative sentiment</strong> in social media, customer reviews, discussion forums and more.</p>
<p>The service evaluates the text and returns a <strong>sentiment score</strong> in the <strong>range of 0 to 1</strong>, with values <strong>closer to 1 being a positive sentiment</strong>. Scores that are close to the <strong>middle of the range (0.5) are considered neutral or indeterminate</strong>.</p>
<p><strong>Indeterminate sentiment</strong> - A score of 0.5 might indicate that the sentiment of the text is indeterminate, and could result from text that does not have sufficient context to discern a sentiment or insufficient phrasing. For example, a list of words in a sentence that has no structure, could result in an indeterminate score. Another example where a score may be 0.5 is in the case where the wrong language code was used. A language code (such as "en" for English, or "fr" for French) is used to inform the service which language the text is in. If you pass text in French but tell the service the language code is en for English, the service will return a score of precisely 0.5.</p>
<h4 id="key-phrase-extraction-not-cleaned">Key phrase extraction (NOT CLEANED)</h4>
<p>Key phrase extraction is the concept of evaluating the text of a document, or documents, and then <strong>identifying the main talking points</strong> of the document(s).</p>
<h4 id="entity-recognition-not-cleaned">Entity recognition (NOT CLEANED)</h4>
<p>You can provide the Language service with unstructured text and it will return a list of entities in the text that it recognizes. The service can also provide links to more information about that entity on the web. An entity is essentially an item of a particular type or a category; and in some cases, subtype,</p>
<h3 id="recognize-and-synthesize-speech">Recognize and synthesize speech</h3>
<p>Is an artificial intelligence (AI) solutions to accept <strong>vocal commands and provide spoken responses</strong>. </p>
<p>To enable this kind of interaction, the AI system must support two capabilities:</p>
<ul>
<li><strong>Speech recognition</strong> - the ability to <strong>detect</strong> and <strong>interpret spoken input</strong> (<strong>speech to text</strong>)</li>
<li><strong>Speech synthesis</strong> - the ability to generate <strong>spoken output</strong> (<strong>text to speech</strong>)</li>
</ul>
<h4 id="speech-recognition">Speech recognition</h4>
<p>Typically, uses those types of models:</p>
<ul>
<li><strong>Acoustic model</strong> to <strong>convert the audio signal</strong> into phonemes (representations of specific sounds)</li>
<li><strong>Language model</strong> to <strong>maps phonemes to words</strong>, usually using a statistical algorithm that predicts the most probable sequence of words based on the phonemes</li>
</ul>
<p>The converted text can be used in such as:</p>
<ul>
<li>Providing <strong>closed captions</strong> for recorded or live videos</li>
<li>Creating a <strong>transcript</strong> of a phone call or meeting</li>
<li>Automated note <strong>dictation</strong></li>
<li><strong>Determining user input</strong> for further processing</li>
</ul>
<h4 id="speech-synthesis">Speech synthesis</h4>
<p>To synthesize speech, the system typically <strong>tokenizes the text</strong> to break it down into individual words, and <strong>assigns phonetic sounds to each word</strong>. It then breaks the phonetic <strong>transcription into prosodic units</strong> (such as phrases, clauses, or sentences) to create phonemes that will be converted to audio format.</p>
<p>The Output of speech synthesis can be used in:</p>
<ul>
<li>Generating <strong>spoken responses</strong> to user input</li>
<li>Creating <strong>voice menus</strong> for telephone systems</li>
<li><strong>Reading</strong> email or text messages aloud in hands-free scenarios</li>
<li><strong>Broadcasting announcements</strong> in public locations, such as railway stations or airports</li>
</ul>
<h4 id="azure-resources-for-the-speech-service">Azure resources for the Speech service</h4>
<p>Microsoft Azure offers both speech recognition and speech synthesis capabilities through:</p>
<ul>
<li>
<p>The <strong>Speech-to-Text</strong> API - is based on the <strong>Universal Language Model</strong>. The model is optimized for two scenarios, <strong>conversational</strong> and <strong>dictation</strong></p>
<ul>
<li><strong>Real-time transcription</strong> - transcribe text in audio streams. Use <strong>real-time</strong> transcription for presentations, demos, or any other scenario where a person is speaking.</li>
<li><strong>Batch transcription</strong> - to transcribe <strong>non-real-time</strong> audio recordings stored on a file share, a remote server, or even on Azure storage. Batch transcription should be run in an <strong>asynchronous* manner because the batch jobs are scheduled on a </strong>best-effort basis**. </li>
</ul>
</li>
<li>
<p>The <strong>Text-to-Speech</strong> API - convert text input to audible speech</p>
<ul>
<li><strong>Speech synthesis voices</strong> -  specify the voice to be used to vocalize the text, including <strong><em>standard voices</em></strong> or <strong><em>neural voices</em></strong> (more natural sounding voice)</li>
</ul>
</li>
</ul>
<p>To use the Speech service in an application, users must create an appropriate resource in your Azure subscription either of the following types of resource:</p>
<ul>
<li>A <strong>Speech resource</strong> - <strong>only</strong> to use the Speech service, or to manage access and <strong>billing for the resource separately</strong> from other services</li>
<li>A <strong>Cognitive Services resource</strong> - to use the Speech service in <strong>combination</strong> with other cognitive services, and to manage access and <strong>billing for these services together</strong></li>
</ul>
<h3 id="translate-text-and-speech">Translate text and speech</h3>
<p>Is to use AI for language translations, sometimes known as <strong>machine translation</strong></p>
<h4 id="literal-and-semantic-translation">Literal and semantic translation</h4>
<p>To translate words to the corresponding word in the target language including the accurate <strong><em>semantic</em></strong> context </p>
<h4 id="text-and-speech-translation">Text and speech translation</h4>
<ul>
<li><strong>Text translation</strong> can be used to <strong>translate documents</strong> from one language to another, translate email communications that come from foreign governments, and even provide the ability to translate web pages on the Internet.</li>
<li><strong>Speech translation</strong> is used to translate between <strong>spoken languages</strong>, sometimes directly (<strong><em>speech-to-speech translation</em></strong>) and sometimes by translating to an intermediary text format (<strong><em>speech-to-text translation</em></strong>).</li>
</ul>
<h4 id="azure-resources-for-translation">Azure resources for translation</h4>
<p>Microsoft Azure provides cognitive services that support translation with the following services:</p>
<ul>
<li>
<p>The <strong>Translator</strong> service - supports <strong>text-to-text</strong> translation</p>
<ul>
<li>The service uses a <strong>Neural Machine Translation (NMT) model</strong> for translation, which analyzes the semantic context of the text and renders a more accurate and complete translation as a result.</li>
<li>Supports text-to-text translation in <strong>more than 60 languages</strong></li>
<li>Optional Configurations: <strong>Profanity filtering</strong> translate <strong><em>without any configuration</em></strong>, <strong>Selective translation</strong>  tag content that <strong>not to translated</strong></li>
</ul>
</li>
<li>
<p>The <strong>Speech</strong> service - enables <strong>speech-to-text</strong> and <strong>speech-to-speech</strong> translation</p>
<ul>
<li><strong>Speech-to-text</strong> - transcribe speech from an audio source to text format </li>
<li><strong>Text-to-speech</strong> - generate spoken audio from a text source</li>
<li><strong>Speech Translation</strong> - translate speech in one language to text or speech in another</li>
<li>Translate speech into <strong>over 60 languages</strong></li>
</ul>
</li>
</ul>
<h3 id="conversational-language-understanding">Conversational Language Understanding</h3>
<p>is to be able to <strong>interpret the semantic meaning</strong> of the input (understand what is being said). Conversational language understanding is supported through the <strong>Language Service</strong> which need to take into account three core concepts:</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="left">Utterances</th>
<th align="left">Entities</th>
<th align="left">Intents</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">definition</td>
<td align="left">Something a user might say. Can be a sentence or words. It consists with an entity and intent</td>
<td align="left">An item to which an utterance refers for determine a command</td>
<td align="left">represents the <strong>purpose</strong>, or goal, expressed in a user's utterance. Can be understand as what the <strong>user want</strong></td>
</tr>
<tr>
<td align="center">example</td>
<td align="left">Switch the fan on.<br/>Turn off the light.</td>
<td align="left">Switch the <strong>fan</strong> on. (Fan is an entity)<br/>Turn off the <strong>light</strong>. (light is an entity)</td>
<td align="left"><strong>Switch</strong> the fan <strong>on</strong>. (TurnON is an intent)<br/><strong>Turn</strong> the light <strong>off</strong>. (TurnOff is an intent)</td>
</tr>
</tbody>
</table>
<p><strong>NOTE</strong></p>
<p>Of special interest is the <strong>None</strong> intent. <strong>Always</strong> consider using the None intent to help <strong>handle utterances</strong> that do <strong>not match</strong> any of the utterances you have entered. The None intent is considered a fallback, and is typically used to <strong>provide a generic response</strong> to users when their requests don't match any other intent.</p>
<h4 id="azure-resources-for-conversational-language-understanding">Azure resources for Conversational Language Understanding</h4>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="left">Language Service</th>
<th align="left">Cognitive Services</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">definition</td>
<td align="left">to <strong>build apps</strong> with natural language understanding capabilities</td>
<td align="left">is a general cognitive services resource that includes Conversational Language Understanding along with many other cognitive services. You can <strong>only</strong> use <strong>this type</strong> of resource <strong>for prediction</strong></td>
</tr>
<tr>
<td align="center">implementation</td>
<td align="left">want to track resource utilization for Language Service use separately from client applications</td>
<td align="left">manage access to all of the cognitive services being used, including the Language Service, through a single endpoint and key.</td>
</tr>
</tbody>
</table>
<h4 id="authoring">Authoring</h4>
<p>is to define the resources to author and train a Conversational Language Understanding application by defining the entities and intents as well as utterances.</p>
<h5 id="creating-intents">Creating intents</h5>
<p>Define intents based on actions a user would want to perform with your application. For each intent, you should include a variety of utterances that provide examples of how a user might express the intent.</p>
<h5 id="creating-entities">Creating entities</h5>
<ul>
<li><strong>Machine-Learned</strong> - Entities that are learned by your model during training from <strong>context in the sample utterances</strong> you provide.</li>
<li><strong>List</strong> -  Entities that are defined as a <strong>hierarchy of lists and sublists</strong>. For example, a device list might include sublists for light and fan. For each list entry, you can specify synonyms, such as lamp for light.</li>
<li><strong>RegEx</strong> - Entities that are defined as a <strong>regular expression</strong> that describes a <strong>pattern</strong> - for example, you might define a pattern like [0-9]{3}-[0-9]{3}-[0-9]{4} for telephone numbers of the form 555-123-4567.</li>
<li><strong>Pattern.any* - Entities that are used with </strong>patterns** to define complex entities that may be hard to extract from sample utterances.</li>
</ul>
<h5 id="training-the-model">Training the model</h5>
<p>Training is the process of <strong>using the sample utterances to teach the model</strong> to match natural language expressions that a user might say to probable intents and entities.</p>
<h4 id="predicting">Predicting</h4>
<p>When satisfied with the results from the training and testing, the model can be published to a Conversational Language Understanding application to a prediction resource for consumption.</p>
<p><strong>Client applications</strong> can use the model by <strong>connecting to the endpoint</strong> for the <strong>prediction resource</strong>, specifying the appropriate <strong>authentication key</strong>; and <strong>submit user input</strong> to get predicted intents and entities. The predictions are returned to the client application, which can then take appropriate action based on the predicted intent.</p>
<h3 id="build-a-bot-with-language-service-and-azure-bot-service">Build a bot with Language Service and Azure Bot Service</h3>
<p>This section is an example to create a <strong>chatbot</strong> for frequently asked questions (<strong>FAQ</strong>) using Language Service and Azure Bot Service based on <strong>existing FAQ documentation</strong> To implement this kind of solution, it is needed to have:</p>
<ul>
<li>A <strong>knowledge base</strong> of question and answer pairs - usually with some built-in natural language processing model to enable questions that can be phrased in multiple ways to be understood with the same semantic meaning.</li>
<li>A <strong>bot service</strong> that provides an interface to the knowledge base through one or more channels.</li>
</ul>
<p>To creating a bot solution on Microsoft Azure, can be done with the use of the following core services</p>
<ul>
<li>The <strong>Language service</strong> using a custom question answering feature that enables you to <strong>create a knowledge base of question and answer pairs</strong> that can be queried using natural language input.</li>
<li><strong>Azure Bot service</strong> a service provides a <strong>framework for developing, publishing, and managing bots</strong> on Azure.</li>
</ul>
<p>???* Note
    The question answering capability in the Language service is a newer version of the QnA Maker service - which is still available as a separate service.</p>
<ol>
<li><strong>Create a custom question answering knowledge base</strong> - by using Language Studio's question answering feature to create, train, publish, and manage knowledge bases.
    1.1 <strong>Provision a Language service Azure resource</strong> in an Azure subscription
    1.2 <strong>Define questions and answer</strong> by using <strong>Language Studio</strong>'s custom question answering feature to create a knowledge base 
    1.3 <strong>Test the knowledge base</strong> by submitting questions and reviewing the answers
    1.4 <strong>Use (deploy) the knowledge base</strong></li>
<li>Build a bot with the Azure Bot Service
    2.1 <strong>Create a bot for the knowledge base</strong> using the Microsoft Bot Framework SDK to write code that controls conversation flow and integrates with your knowledge base
    2.2 Extend and <strong>configure</strong> the bot
    2.3 <strong>Connect channels</strong> and making it possible for users to interact with the bot</li>
</ol>
<h2 id="anomaly-detection_1">Anomaly Detection</h2>
<p><strong>Anomalies* are values that are </strong>outside the expected values or range** of values.</p>
<p>Is a technique to determine whether values in a series are <strong>within expected parameters</strong>.</p>
<p>Anomaly Detector is a part of the <strong>Decision Services</strong> category within <strong>Azure Cognitive Services</strong>.</p>
<p>The <strong>main parameter</strong> you need to customize is <strong>“Sensitivity”</strong>, which is from 1 to 99 to adjust the outcome to fit the scenario. The service can detect anomalies in historical time series data and also in real-time data such as streaming input from IoT devices, sensors, or other streaming input sources.</p>
<h4 id="how-anomaly-detector-works">How Anomaly Detector works</h4>
<p>The Anomaly Detector service <strong>identifies anomalies that exist outside the scope of a boundary</strong>. By default, the upper and lower boundaries for anomaly detection are calculated using concepts known as <strong>expectedValue</strong>, <strong>upperMargin</strong>, and <strong>lowerMargin</strong>. The boundaries can be adjusted by applying a <strong>marginScale</strong> to the upper and lower margins as demonstrated by the following formula.</p>
<div class="highlight"><pre><span></span><code>upperBoundary = expectedValue + (100 - marginScale) * upperMargin
</code></pre></div>
<h4 id="data-format">Data format</h4>
<p>The Anomaly Detector service accepts data in <strong>JSON</strong> format. The service will support a <strong>maximum of 8640 data points</strong>.</p>
<h4 id="data-consistency-recommendations">Data consistency recommendations</h4>
<ul>
<li>If the sampling occurs <strong>every few minutes</strong> and has <strong>less than 10%</strong> of the expected number of points <strong>missing</strong>. In this case, the <strong>impact should be negligible</strong> on the detection results.</li>
<li>If you have <strong>more than 10% missing</strong>, there are <strong>options</strong> to help <strong>"fill"</strong> the data set. Consider <strong>using a linear interpolation</strong> method to fill in the missing values and complete the data set. This will fill gaps with evenly distributed values.</li>
</ul>
<h4 id="batch-detection">Batch detection</h4>
<p>Batch detection involves applying the algorithm to an <strong>entire data series at one time</strong>. Batch detection is best used when your data contains:</p>
<ul>
<li><strong>Flat trend time series data</strong> with occasional spikes or dips</li>
<li>
<p><strong>Seasonal time series data</strong> with occasional anomalies</p>
<ul>
<li><strong>Seasonality</strong> is considered to be a pattern in your data, that <strong>occurs at regular intervals</strong>.</li>
</ul>
</li>
</ul>
<h4 id="real-time-detection">Real-time detection</h4>
<p>Real-time detection uses <strong>streaming data</strong> by <strong>comparing previously seen data</strong> points to the last data point to determine if your latest one is an anomaly.</p>
<p><br></p>
<p><a name=''>&lt;&gt;</p>
<h2 id="explore-knowledge-mining">Explore knowledge mining</h2>
<p>Knowledge mining is the term used to describe solutions that involve <strong>extracting information from large volumes of often unstructured data</strong>.</p>
<h3 id="azure-cognitive-search">Azure Cognitive Search</h3>
<p><strong>Azure Cognitive Search</strong> provides the infrastructure and <strong>tools to create search solutions</strong> that extract data from a variety of structured, semi-structured, and non-structured documents. It's a Platform as a Service (<strong>PaaS</strong>) solution</p>
<h4 id="azure-cognitive-search-features">Azure Cognitive Search features</h4>
<p>It is a highly available platform offering a <strong>99.9% uptime SLA</strong> available for cloud and on-premises assets. Comes with the following features:</p>
<ul>
<li><strong>Data from any source</strong>: accepts data from any source provided in <strong>JSON</strong> format, with auto crawling support for selected data sources in Azure.</li>
<li><strong>Full text search and analysis</strong>: offers full text search capabilities supporting both simple query and full Lucene query syntax.</li>
<li><strong>AI powered search</strong>: has Cognitive AI <strong>capabilities</strong> built in for image and text analysis from <strong>raw content</strong>.</li>
<li><strong>Multi-lingual</strong>: <strong>linguistic analysis for 56 languages</strong> to intelligently handle phonetic matching or language-specific linguistics.</li>
<li><strong>Geo-enabled</strong>: supports geo-search filtering based on proximity to a physical location.</li>
<li><strong>Configurable user experience</strong>: several features to improve the user experience including autocomplete, autosuggest, pagination, and hit highlighting.</li>
</ul>
<h5 id="built-in-skills">Built in skills</h5>
<p>Built-in skills are based on <strong>pre-trained models from Microsoft</strong>, which means <strong>users can't train the model</strong> using their own training data. Built-in skills fall into these categories:</p>
<ul>
<li>
<p><strong>Natural language processing skills</strong>: with these skills, <strong>unstructured text</strong> is mapped as searchable and filterable fields in an <strong>index</strong>. Some examples include:</p>
<ul>
<li><strong>Key Phrase Extraction</strong>: uses a pre-trained model to <strong>detect important phrases</strong> based on term placement, linguistic rules, proximity to other terms, and how unusual the term is within the source data. </li>
<li><strong>Text Translation Skill</strong>: uses a pre-trained model to <strong>translate</strong> the input text into a variety of languages for normalization or localization use cases.</li>
</ul>
</li>
<li>
<p><strong>Image processing skills</strong>: creates <strong>text representations of image</strong> content, making it searchable using the query capabilities of Azure Cognitive Search. Some examples include:</p>
<ul>
<li><strong>Image Analysis Skill</strong>: uses an <strong>image detection</strong> algorithm to identify the content of an image and <strong>generate a text description</strong>. </li>
<li><strong>Optical Character Recognition (OCR) Skill</strong>: allows <strong>extracting printed or handwritten text from images</strong>, such as photos of street signs and products, as well as from documents—invoices, bills, financial reports, articles, and more.</li>
</ul>
</li>
</ul>
<h5 id="indexes">Indexes</h5>
<p>Index can be thought of as a container of searchable documents</p>
<ul>
<li>Index schema:</li>
<li>Index Attribute:</li>
</ul>
<p><strong> Azure Indexer: to </strong>export a document<strong> from their original file type </strong>to JSON**.</p>
<ul>
<li><strong>Push method</strong>: <strong>JSON data is pushed into a search index via either the REST API</strong> or the .NET SDK. Pushing data has the most flexibility as it has no restrictions on the data source type, location, or frequency of execution.</li>
<li><strong>Pull method</strong>: Search service indexers can <strong>pull data from popular Azure data sources</strong>, and if necessary, export that data into JSON if it isn't already in that format.</li>
</ul>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2020 - 2022 Mark Schwarz
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["search.suggest", "search.highlight", "search.tabs.link", "content.tabs.link", "navigation.indexes", "navigation.tabs", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.cefbb252.min.js"}</script>
    
    
      <script src="../../../assets/javascripts/bundle.a5f8ea78.min.js"></script>
      
    
  </body>
</html>